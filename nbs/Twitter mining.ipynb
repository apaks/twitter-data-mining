{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter data mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://marcobonzanini.com/2015/03/02/mining-twitter-data-with-python-part-1/  \n",
    "https://towardsdatascience.com/mining-twitter-data-ba4e44e6aecc  \n",
    "https://towardsdatascience.com/@rickykim78  \n",
    "https://towardsdatascience.com/tweepy-for-beginners-24baf21f2c25  \n",
    "https://amueller.github.io/word_cloud/auto_examples  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install tweepy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tweepy\n",
    "import json \n",
    "import datetime\n",
    "import time\n",
    "import seaborn as sns\n",
    "import wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auth_ap import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the authentication object\n",
    "auth = tweepy.OAuthHandler(auth_ap.consumer_key, auth_ap.consumer_secret)\n",
    "# Setting your access token and secret\n",
    "auth.set_access_token(auth_ap.access_token, auth_ap.access_token_secret)\n",
    "# Creating the API object while passing in auth information\n",
    "api = tweepy.API(auth) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the API object to get tweets from your timeline, and storing it in a variable called public_tweets\n",
    "public_tweets = api.home_timeline()\n",
    "# foreach through all tweets pulled\n",
    "for tweet in public_tweets:\n",
    "   # printing the text stored inside the tweet object\n",
    "   print (tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What data are available from tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = tweet\n",
    "json_str = json.dumps(status._json)\n",
    "\n",
    "#deserialise string into python object\n",
    "parsed = json.loads(json_str)\n",
    "\n",
    "print(json.dumps(parsed, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetMiner(object):\n",
    "    import auth_ap\n",
    "    # number of tweets per one pull; there is limit on this\n",
    "    result_limit    =   20    \n",
    "    data            =   []\n",
    "    api             =   False\n",
    "    \n",
    "    twitter_keys = {\n",
    "        'consumer_key':        auth_ap.consumer_key,\n",
    "        'consumer_secret':     auth_ap.consumer_secret,\n",
    "        'access_token_key':    auth_ap.access_token,\n",
    "        'access_token_secret': auth_ap.access_token_secret\n",
    "    }\n",
    "    \n",
    "    \n",
    "    def __init__(self, keys_dict=twitter_keys, api=api, result_limit = 20):\n",
    "        \n",
    "        self.twitter_keys = keys_dict\n",
    "        \n",
    "        auth = tweepy.OAuthHandler(keys_dict['consumer_key'], keys_dict['consumer_secret'])\n",
    "        auth.set_access_token(keys_dict['access_token_key'], keys_dict['access_token_secret'])\n",
    "        \n",
    "        self.api = tweepy.API(auth)\n",
    "        self.twitter_keys = keys_dict\n",
    "        \n",
    "        self.result_limit = result_limit\n",
    "\n",
    "        \n",
    "    def tweets_to_dict(self, statuses, incl_retweets = True):\n",
    "        data = []\n",
    "        for item in statuses:\n",
    "            \n",
    "            mined = {\n",
    "                'tweet_id':        item.id,\n",
    "                'name':            item.user.name,\n",
    "                'screen_name':     item.user.screen_name,            # username\n",
    "                \"followers_count\": item.user.followers_count,\n",
    "                \"friends_count\":   item.user.friends_count,\n",
    "                'retweet_count':   item.retweet_count,\n",
    "                'text':            item.full_text,\n",
    "                'mined_at':        datetime.datetime.now(),\n",
    "                'created_at':      item.created_at,\n",
    "                'favourite_count': item.favorite_count,              # # of likes\n",
    "                'hashtags':        item.entities['hashtags'],\n",
    "                'status_count':    item.user.statuses_count,         # # of tweeets\n",
    "                'location':        item.place,\n",
    "                'source_device':   item.source\n",
    "            }\n",
    "            if incl_retweets:\n",
    "                try:\n",
    "                    mined['retweet_text'] = item.retweeted_status.full_text\n",
    "                except:\n",
    "                    mined['retweet_text'] = 'None'\n",
    "                try:\n",
    "                    mined['quote_text'] = item.quoted_status.full_text\n",
    "                    mined['quote_screen_name'] = status.quoted_status.user.screen_name\n",
    "                except:\n",
    "                    mined['quote_text'] = 'None'\n",
    "                    mined['quote_screen_name'] = 'None'\n",
    "            data.append(mined)\n",
    "        return data, statuses[-1].id\n",
    "        \n",
    "    def mine_tweets_user(self, user=\"\",\n",
    "                         incl_retweets = True, last_tweet_id  =  False,\n",
    "                         max_pages=17):\n",
    "\n",
    "        data_page = []\n",
    "        # keep track of last tweet id\n",
    "        \n",
    "        # multiply by the # of result_limit = total tweets\n",
    "        page           =  1\n",
    "        \n",
    "        while page <= max_pages:\n",
    "            if last_tweet_id:\n",
    "                statuses   =   self.api.user_timeline(screen_name = user,\n",
    "                                                     count = self.result_limit,\n",
    "                                                     # get tweets older than last retrieved ones  \n",
    "                                                     max_id = last_tweet_id - 1,\n",
    "                                                     tweet_mode = 'extended',\n",
    "                                                     include_retweets = incl_retweets\n",
    "                                                    )        \n",
    "            else:\n",
    "                statuses   =   self.api.user_timeline(screen_name=user,\n",
    "                                                        count = self.result_limit,\n",
    "                                                        tweet_mode = 'extended',\n",
    "                                                        include_retweets = incl_retweets)\n",
    "                \n",
    "            data, last_tweet_id = self.tweets_to_dict(statuses, incl_retweets)\n",
    "            # need item to keep track of the last tweet id\n",
    "            \n",
    "            \n",
    "            data_page.extend(data)\n",
    "            page += 1\n",
    "        # returns list of dict\n",
    "        return data_page, last_tweet_id\n",
    "    \n",
    "    def mine_tweets_keyword(self, query = \"\", language = 'en',\n",
    "                         incl_retweets = True, last_tweet_id  =  False,\n",
    "                         max_pages=17):\n",
    "\n",
    "        data_page           =  []\n",
    "        # keep track of last tweet id\n",
    "        \n",
    "        # multiply by the # of result_limit = total tweets\n",
    "        page           =  1\n",
    "        \n",
    "        while page <= max_pages:\n",
    "            if last_tweet_id:\n",
    "                statuses   =   self.api.search(q = query, lang = language,\n",
    "                                                     count = self.result_limit,\n",
    "                                                     # get tweets older than last retrieved ones  \n",
    "                                                     max_id = last_tweet_id - 1,\n",
    "                                                     tweet_mode = 'extended',\n",
    "                                                     include_retweets = incl_retweets\n",
    "                                                    )        \n",
    "            else:\n",
    "                statuses   =   self.api.search(q = query, lang = language,\n",
    "                                                        count = self.result_limit,\n",
    "                                                        tweet_mode = 'extended',\n",
    "                                                        include_retweets = incl_retweets)\n",
    "    \n",
    "    \n",
    "            data, last_tweet_id = self.tweets_to_dict(statuses, incl_retweets)\n",
    "            # need item to keep track of the last tweet id\n",
    "            \n",
    "#             print (len(data))\n",
    "            data_page.extend(data)\n",
    "                \n",
    "            page += 1\n",
    "        # returns list of dict\n",
    "        return data_page, last_tweet_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_id = False\n",
    "miner = TweetMiner(result_limit = 1)\n",
    "mined_tweets, last_tweet_id = miner.mine_tweets_user(user='nytimes', max_pages = 17, \n",
    "                                                         last_tweet_id = last_id, incl_retweets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner = TweetMiner(result_limit = 5)\n",
    "last_id = False\n",
    "for i in range(2):\n",
    "    \n",
    "    mined_tweets, last_tweet_id = miner.mine_tweets_user(user='nytimes', max_pages = 17, \n",
    "                                                         last_tweet_id = last_id, incl_retweets=True)\n",
    "    last_id = last_tweet_id\n",
    "#     mined_tweets_df = pd.DataFrame(mined_tweets)\n",
    "    print (mined_tweets[0]['retweet_text'])\n",
    "    print (\"last id\", last_id)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mined_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tweets = api.user_timeline('nytimes',count=3,tweet_mode='extended')\n",
    "for tweet in search_tweets:\n",
    "    if 'retweeted_status' in tweet._json:\n",
    "        print(tweet._json['retweeted_status']['full_text'])\n",
    "    else:\n",
    "        print(tweet.full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make call every 15 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "handle_list= ['list of handles you want the timelines of']\n",
    "\n",
    "twitter_dict={}\n",
    "counter=0\n",
    "\n",
    "for name in handle_list:\n",
    "    try:\n",
    "      twitter_dict[name]=[]\n",
    "      twitter_dict[name].append(miner.mine_user_tweets(user=name, max_pages=17))\n",
    "      counter = counter +1\n",
    "      if counter%40==0:\n",
    "        time.sleep(900) #15 minute sleep time\n",
    "    #if name invalid print name and remove key\n",
    "    except:\n",
    "      print(name, 'is invalid or locked')\n",
    "      twitter_dict.pop(name)\n",
    "    \n",
    "all_tweets=pd.concat([pd.DataFrame(twitter_dict[i][0]) for i in twitter_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mine by keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "last_id = False\n",
    "mined_tweets, last_tweet_id = miner.mine_tweets_keyword(query='AR/VR', language = 'en', \n",
    "                               last_tweet_id = last_id, incl_retweets = True, max_pages = 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mined_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner = TweetMiner(result_limit = 100)\n",
    "counter = 1\n",
    "ls = []\n",
    "last_id = False\n",
    "while counter < 15:\n",
    "    print (counter)\n",
    "    try:\n",
    "        \n",
    "        mined_tweets, last_tweet_id = miner.mine_tweets_keyword(query='AR/VR', language = 'en', \n",
    "                                       last_tweet_id = last_id, incl_retweets = True, max_pages = 34)\n",
    "        last_id = last_tweet_id\n",
    "\n",
    "        ls.extend(mined_tweets)\n",
    "    except:\n",
    "        print (\"Limit is reached\")\n",
    "        break\n",
    "    if i % 4 == 0:\n",
    "        mined_tweets_df = pd.DataFrame(ls)\n",
    "        mined_tweets_df.to_pickle(\"twitter{0}.pkl\".format(i))\n",
    "        time.sleep(16*60) #15 minute sleep time\n",
    "        print (i)\n",
    "        ls = []\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get replies to tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try twarc Python package replies  \n",
    "It might be interesting to look at tweets that got many replies  \n",
    "What type of tweets get what type of replies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the authentication object\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "# Setting your access token and secret\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "# Creating the API object while passing in auth information\n",
    "api = tweepy.API(auth) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(10000, 10100):    \n",
    "    user = df_tweets.iloc[idx].screen_name\n",
    "    tweet_id = df_tweets.iloc[idx].tweet_id\n",
    "    max_id = None\n",
    "\n",
    "    replies=[]\n",
    "    for tweet in tweepy.Cursor(api.search, q='to:' + user, since_id = tweet_id, max_id = max_id, timeout=999999).items(100):\n",
    "        if hasattr(tweet, 'in_reply_to_status_id_str'):\n",
    "            if (tweet.in_reply_to_status_id_str==tweet_id):\n",
    "                replies.append(tweet)\n",
    "    \n",
    "    if len(replies) > 0:\n",
    "        print (len(replies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.iloc[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = tweet.user.screen_name\n",
    "tweet_id = tweet.id\n",
    "max_id = None\n",
    "\n",
    "while True:\n",
    "\n",
    "    try:\n",
    "        replies = t.GetSearch(raw_query=q, since_id=tweet_id, max_id=max_id, count=100)\n",
    "    except twitter.error.TwitterError as e:\n",
    "        logging.error(\"caught twitter api error: %s\", e)\n",
    "        time.sleep(60)\n",
    "        continue\n",
    "    for reply in replies:\n",
    "        logging.info(\"examining: %s\" % tweet_url(reply))\n",
    "        if reply.in_reply_to_status_id == tweet_id:\n",
    "            logging.info(\"found reply: %s\" % tweet_url(reply))\n",
    "            yield reply\n",
    "            # recursive magic to also get the replies to this reply\n",
    "            for reply_to_reply in get_replies(reply):\n",
    "                yield reply_to_reply\n",
    "        max_id = reply.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import operator \n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=True):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via', 'vr', 'ar', \n",
    "                '#vr', '#ar', \"#virtualreality\", \"#augmentedreality\", '’', '#mr', '#ai', '#ml', '#3d',\n",
    "                '\"', 'virtual', 'augmented', \"cc\", \"amp\" ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_terms = Counter()\n",
    "count_hash = Counter()\n",
    "count_users = Counter()\n",
    "for idx in range(df_tweets.shape[0]):\n",
    "    text = df_tweets.iloc[idx].retweet_text \n",
    "    if text == \"None\":\n",
    "        text = df_tweets.iloc[idx].text\n",
    "    # Create a list with all the terms\n",
    "#     terms_all = [term for term in preprocess(text) if term not in stop]\n",
    "    terms_hash = [term for term in preprocess(text) \n",
    "              if term not in stop and term.startswith('#')]\n",
    "    terms_only = [term for term in preprocess(text) \n",
    "              if term not in stop and\n",
    "              not term.startswith(('#', '@'))] \n",
    "    users = [term for term in preprocess(text) \n",
    "              if term not in stop and term.startswith('@')]\n",
    "    # Update the counter\n",
    "    count_users.update(users)\n",
    "    count_hash.update(terms_hash)\n",
    "    count_terms.update(terms_only)\n",
    "    # Print the first 5 most frequent words\n",
    "print(count_users.most_common(10))\n",
    "print(count_hash.most_common(10))\n",
    "print(count_terms.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.ogrid[:300, :300]\n",
    "\n",
    "mask = (x - 150) ** 2 + (y - 150) ** 2 > 130 ** 2\n",
    "mask = 255 * mask.astype(int)\n",
    "\n",
    "_input = count_terms\n",
    "wc = WordCloud( mask = mask, contour_width=3, contour_color= 'steelblue',\n",
    "                background_color ='white', max_font_size=50, \n",
    "                max_words=200, random_state=42, \n",
    "                min_font_size = 10).generate_from_frequencies(_input) \n",
    "  \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc, interpolation='bilinear') \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Woedcloud with mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "path_image = r\"C:\\Users\\Chub_lab\\Desktop\\V0D-sLDA.jpeg.jpg\"\n",
    "mask = np.array(Image.open(path_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_input = count_hash\n",
    "wc = WordCloud( mask = mask, contour_width=3, contour_color= 'steelblue',\n",
    "                background_color ='white', max_font_size=100, \n",
    "                max_words=200, random_state=42, \n",
    "                min_font_size = 10).generate_from_frequencies(_input) \n",
    "  \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc, interpolation='bilinear') \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term co-occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams \n",
    "count_terms_bigrams = Counter()\n",
    "\n",
    "for idx in range(df_tweets.shape[0]-1000):\n",
    "    text = df_tweets.iloc[idx].retweet_text \n",
    "    if text == \"None\":\n",
    "        text = df_tweets.iloc[idx].text\n",
    "    # Create a list with all the terms\n",
    "\n",
    "    terms_only = [term for term in preprocess(text) \n",
    "              if term not in stop and\n",
    "              not term.startswith(('#', '@'))] \n",
    "    terms_bigram = bigrams(terms_only)\n",
    "    # Update the counter\n",
    "    count_terms_bigrams.update(terms_bigram)\n",
    "    # Print the first 5 most frequent words\n",
    "print (count_terms_bigrams.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# remember to include the other import from the previous post\n",
    " \n",
    "com = defaultdict(lambda : defaultdict(int))\n",
    "for idx in range(df_tweets.shape[0]):\n",
    "    text = df_tweets.iloc[idx].retweet_text \n",
    "    if text == \"None\":\n",
    "        text = df_tweets.iloc[idx].text\n",
    "    # Create a list with all the terms\n",
    "\n",
    "    terms_only = [term for term in preprocess(text) \n",
    "              if term not in stop and\n",
    "              not term.startswith(('#', '@'))]  \n",
    "\n",
    "    # Build co-occurrence matrix\n",
    "    for i in range(len(terms_only)-1):            \n",
    "        for j in range(i+1, len(terms_only)):\n",
    "            w1, w2 = sorted([terms_only[i], terms_only[j]])                \n",
    "            if w1 != w2:\n",
    "                com[w1][w2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_max = []\n",
    "# For each term, look for the most common co-occurrent terms\n",
    "for t1 in com:\n",
    "    t1_max_terms = sorted(com[t1].items(), key=operator.itemgetter(1), reverse=True)[:5]\n",
    "    for t2, t2_count in t1_max_terms:\n",
    "        com_max.append(((t1, t2), t2_count))\n",
    "# Get the most frequent co-occurrences\n",
    "terms_max = sorted(com_max, key=operator.itemgetter(1), reverse=True)\n",
    "print(terms_max[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term freqeuncy over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {}\n",
    "target_ls = ['#stpiindia', '#fintech', \"#blockchain\", \"#iot\"]\n",
    "for trg in target_ls:\n",
    "    my_dict[trg] = []\n",
    "\n",
    "# f is the file pointer to the JSON data set\n",
    "for idx in range(df_tweets.shape[0]):\n",
    "    tweet = df_tweets.iloc[idx]\n",
    "    text = tweet.retweet_text \n",
    "    if text == \"None\":\n",
    "        text = df_tweets.iloc[idx].text\n",
    "    # Create a list with all the terms\n",
    "\n",
    "    terms_only = [term for term in preprocess(text) \n",
    "              if term not in stop and\n",
    "              term.startswith(('#'))]  \n",
    "    # track when the hashtag is mentioned\n",
    "    for trg in target_ls: \n",
    "        if trg in terms_only:\n",
    "            my_dict[trg].append(tweet['created_at'])\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "for key in my_dict.keys():\n",
    "    tmp = my_dict[key]\n",
    "    ones = [1]*len(tmp)\n",
    "    idx = pd.DatetimeIndex(tmp)\n",
    "    tmp = pd.Series(ones, index=idx)\n",
    "    tmp = tmp.resample('30Min').sum().fillna(0).reset_index()\n",
    "    tmp.columns = ['date', 'freq']\n",
    "    tmp.loc[:, 'target'] = key\n",
    "    ls.append(tmp)\n",
    "df_target_tc = pd.concat(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot tweet frequency over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.dates import DateFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('poster')\n",
    "f, ax = plt.subplots(figsize =(8, 6))\n",
    "ax = sns.lineplot(data = df_target_tc, x = 'date', y = 'freq', hue = 'target')\n",
    "sns.despine()\n",
    "\n",
    "date_form = DateFormatter(\"%m-%d\")\n",
    "ax.xaxis.set_major_formatter(date_form)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Tweet count')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.xticks(rotation = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sian = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_list(dict1):\n",
    "    dictlist = list()\n",
    "    for key, value in dict1.items():\n",
    "        temp = [key,value]\n",
    "        dictlist.append(temp)\n",
    "    return dictlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_compound = {}\n",
    "tweet_sentim = []\n",
    "for idx in range(df_tweets.shape[0]):\n",
    "    text = df_tweets.iloc[idx].retweet_text \n",
    "    if text == \"None\":\n",
    "        text = df_tweets.iloc[idx].text\n",
    "    tweet_id = df_tweets.iloc[idx].tweet_id\n",
    "    d_compound[tweet_id] = sian.polarity_scores(text)['compound']\n",
    "#     dict_res = dict_to_list(sian.polarity_scores(text))\n",
    "    #nltk.sentiment.util.demo_vader_instance(_)\n",
    "#     tweet_sentim.append([text, dict_res[0][1], dict_res[1][1], dict_res[2][1], dict_res[3][1]])\n",
    "\n",
    "# df_sentim = pd.DataFrame(tweet_sentim)\n",
    "# df_sentim.columns = ['text', 'neg', 'neu', 'pos', 'compound']\n",
    "# df_sentim.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://github.com/cjhutto/vaderSentiment  \n",
    "    The compound score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate.\n",
    "\n",
    "    It is also useful for researchers who would like to set standardized thresholds for classifying sentences as either positive, neutral, or negative. Typical threshold values (used in the literature cited on this page) are:\n",
    "\n",
    "        positive sentiment: compound score >= 0.05\n",
    "        neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
    "        negative sentiment: compound score <= -0.05\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentim = df_sentim.drop_duplicates()\n",
    "df_sentim = df_sentim.sort_values(['compound'], ascending=False)\n",
    "df_sentim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    text = df_sentim[df_sentim['compound'] < -0.5].iloc[i].text\n",
    "    print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_sentim['compound'])\n",
    "sns.despine()\n",
    "plt.axvline(x = -0.05, linestyle = '--', color = 'k')\n",
    "plt.axvline(x = 0.05, linestyle = '--', color = 'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.loc[:, 'compound'] = df_tweets.tweet_id.map(d_compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dev = df_tweets.groupby(['source_device']).tweet_id.count().reset_index()\n",
    "toi = src_dev[src_dev.tweet_id > 100].source_device.unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the soruce device correlate with senimnet polarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_input = df_tweets[df_tweets.source_device.isin(toi)].sort_values(by = ['source_device'])\n",
    "sns.catplot(data = _input, y = 'source_device', x = 'compound', kind = 'violin',\n",
    "           height = 6, aspect = 1.4, orient = 'h')\n",
    "# plt.xticks(rotation= 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster users by bio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is better to save tweets to .json and load what you need for the analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def get_parser():\n",
    "    parser = ArgumentParser(\"Clustering for followers\")\n",
    "    parser.add_argument('--filename') \n",
    "    parser.add_argument('--k', type=int) \n",
    "    parser.add_argument('--min-df', type=int, default=2) \n",
    "    parser.add_argument('--max-df', type=float, default=0.8) \n",
    "    parser.add_argument('--max-features', type=int, default=None) \n",
    "    parser.add_argument('--no-idf', dest='user_idf', default=True, action='store_false') \n",
    "    parser.add_argument('--min-ngram', type=int, default=1) \n",
    "    parser.add_argument('--max-ngram', type=int, default=1) \n",
    "    return parserif __name__ == '__main__':\n",
    "    parser = get_parser()\n",
    "    args = parser.parse_args()\n",
    "    if args.min_ngram > args.max_ngram:\n",
    "        print(\"Error: incorrect value for --min--ngram ({}): it cant be higher than \\\n",
    "        --max--value ({})\".format(args.min_ngram, args.max_ngram))\n",
    "        sys.exit(1)\n",
    "    with open(args.filename) as f:\n",
    "        #load datausers = []\n",
    "        for line in f:\n",
    "            profile = json.loads(line)\n",
    "            users.append(profile['description'])\n",
    "        #create vectorizer\n",
    "        vectorizer = TfidfVectorizer(max_df=args.max_df,\n",
    "                                    min_df=args.min_df,\n",
    "                                    max_features=args.max_features,\n",
    "                                    stop_words='english',\n",
    "                                    ngram_range=(args.min_ngram, args.max_ngram),\n",
    "                                    use_idf=args.user_idf)#fit data\n",
    "        X = vectorizer.fit_transform(users)\n",
    "        print(\"Data dimensions: {}\".format(X.shape))#perform clustering\n",
    "        km = KMeans(n_clusters=args.k)\n",
    "        km.fit(X)\n",
    "        clusters = defaultdict(list)\n",
    "        for i, label in enumerate(km.labels_):\n",
    "            clusters[label].append(users[i])#print 10 user description of this clusterfor label, description in clusters.items():\n",
    "            print(\"--------- Cluster {}\".format(label+i))\n",
    "            for desc in description[:10]:\n",
    "                print(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
